{"config":{"indexing":"full","jieba_dict":null,"jieba_dict_user":null,"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Welcome to Demystifying dbt online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino, ksqlDB and recently dbt, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring dbt as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>   \"The Internals Of\" series <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a closer look at dbt \ud83d\udd25</p>  <p>Last update: 2022-07-09</p>","title":"Demystifying dbt"},{"location":"001/","text":"","title":"Day 1: Installing dbt"},{"location":"001/#pyenv","text":"<ul> <li>pyenv for Simple Python Version Management</li> <li>Command Reference</li> </ul> <pre><code>$ pyenv --version\npyenv 2.3.2\n\n$ pyenv install 3.10:latest\nDownloading Python-3.10.5.tar.xz...\n-&gt; https://www.python.org/ftp/python/3.10.5/Python-3.10.5.tar.xz\nInstalling Python-3.10.5...\nInstalled Python-3.10.5 to /Users/jacek/.pyenv/versions/3.10.5\n\n$ pyenv rehash\n\n$ pyenv local 3.10.5\n\n$ python --version\nPython 3.10.5\n\n$ python3.10 --version\nPython 3.10.5\n\n$ pyenv version\n3.10.5 (set by /Users/jacek/dev/sandbox/dbt-sandbox/.python-version)\n</code></pre>","title":"pyenv"},{"location":"001/#pyenv-virtualenv","text":"<p>pyenv-virtualenv is a <code>pyenv</code> plugin for managing virtualenvs and conda environments for Python on UNIX-like systems.</p> <pre><code>$ pyenv virtualenv dbt\n\n$ pyenv virtualenvs --skip-aliases\ndbt (created from /usr/local/opt/python@3.9)\n\n$ pyenv local dbt\n\n$ cat .python-version\ndbt\n</code></pre>","title":"pyenv-virtualenv"},{"location":"001/#install-dbt-incl-databricks-adapter-plugin","text":"<p>This is a modified version of the official Install dbt from source to avoid polluting system-wide configuration.</p> <pre><code>$ pip install --upgrade pip wheel setuptools\nSuccessfully installed pip-22.1.2 setuptools-62.6.0 wheel-0.37.1\n\n$ cat requirements.txt\ndbt-core\ndbt-postgres\ndbt-databricks\n\n$ pip install -r requirements.txt\n...\nSuccessfully installed Babel-2.10.3 Jinja2-2.11.3 MarkupSafe-2.0.1 agate-1.6.3 attrs-21.4.0 certifi-2022.6.15 cffi-1.15.1 charset-normalizer-2.1.0 click-8.1.3 colorama-0.4.4 databricks-sql-connector-2.0.2 dbt-core-1.1.1 dbt-databricks-1.1.0 dbt-extractor-0.4.1 dbt-postgres-1.1.1 dbt-spark-1.1.0 future-0.18.2 hologram-0.0.15 idna-3.3 isodate-0.6.1 jsonschema-3.2.0 leather-0.3.4 logbook-1.5.3 mashumaro-2.9 minimal-snowplow-tracker-0.0.2 msgpack-1.0.4 networkx-2.8.3 numpy-1.23.0 packaging-21.3 pandas-1.4.3 parsedatetime-2.4 psycopg2-binary-2.9.3 pyarrow-8.0.0 pycparser-2.21 pyparsing-3.0.9 pyrsistent-0.18.1 python-dateutil-2.8.2 python-slugify-6.1.2 pytimeparse-1.1.8 pytz-2022.1 pyyaml-6.0 requests-2.28.1 six-1.16.0 sqlparams-4.0.0 sqlparse-0.4.2 text-unidecode-1.3 thrift-0.16.0 typing-extensions-4.3.0 urllib3-1.26.9 werkzeug-2.1.2\n\n$ pip list dbt | egrep \"^dbt-\" | sort\ndbt-core                 1.1.1\ndbt-databricks           1.1.0\ndbt-extractor            0.4.1\ndbt-postgres             1.1.1\ndbt-spark                1.1.0\n</code></pre>","title":"Install dbt (incl. Databricks adapter plugin)"},{"location":"001/#install-from-sources","text":"<p>Clone the <code>dbt-databricks</code> adapter from https://github.com/databricks/dbt-databricks and install from the directory.</p> <pre><code>git clone https://github.com/databricks/dbt-databricks\ncd dbt-databricks\ngco v1.1.0\ngswc v1.1.0\npip install .\n</code></pre> <pre><code>Successfully installed dbt-databricks-1.1.0\n</code></pre>","title":"Install from Sources"},{"location":"002/","text":"","title":"Day 2: Configuring dbt Profile"},{"location":"002/#postgres-profile","text":"<ol> <li>Configuring your profile</li> <li>Postgres Profile</li> </ol>","title":"Postgres Profile"},{"location":"002/#profilesyml","text":"<p>profiles.yml Reference</p> ~/.dbt/profiles.yml<pre><code>postgres_at_my_company:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      user: postgres\n      password: xxx\n      database: postgres\n      schema: postgres\n      host: localhost\n      port: 5432\n</code></pre>","title":"profiles.yml"},{"location":"002/#dbt_projectyml","text":"<p>dbt_project.yml Reference</p> dbt_project.yml<pre><code>name: 'dbt_postgres_demo'\n\nconfig-version: 2\nversion: 1.0.0\n\nprofile: 'postgres_at_my_company'\n\nrequire-dbt-version: \"&gt;=1.1.1\"\n</code></pre>","title":"dbt_project.yml"},{"location":"002/#start-postgres","text":"<p>Docker official image of PostgreSQL</p> <pre><code>docker run \\\n  -d \\\n  --rm \\\n  --name postgres \\\n  -e POSTGRES_PASSWORD=xxx \\\n  -p \"5432:5432\" \\\n  postgres\n</code></pre> <pre><code>$ docker exec -it postgres psql -U postgres\npsql (14.4 (Debian 14.4-1.pgdg110+1))\nType \"help\" for help.\n\npostgres=# \\conninfo\nYou are connected to database \"postgres\" as user \"postgres\" via socket in \"/var/run/postgresql\" at port \"5432\".\n\npostgres=# \\l\n                                 List of databases\n   Name    |  Owner   | Encoding |  Collate   |   Ctype    |   Access privileges\n-----------+----------+----------+------------+------------+-----------------------\n postgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\n template0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n           |          |          |            |            | postgres=CTc/postgres\n template1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n           |          |          |            |            | postgres=CTc/postgres\n(3 rows)\n\npostgres=# \\dn+\n                          List of schemas\n  Name  |  Owner   |  Access privileges   |      Description\n--------+----------+----------------------+------------------------\n public | postgres | postgres=UC/postgres+| standard public schema\n        |          | =UC/postgres         |\n(1 row)\n</code></pre>","title":"Start Postgres"},{"location":"002/#dbt-debug","text":"<p>Use dbt debug to validate warehouse credentials</p> <pre><code>$ dbt debug --config-dir\nTo view your profiles.yml file, run:\n\nopen /Users/jacek/.dbt\n\n$ dbt debug\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /Users/jacek/dev/sandbox/dbt-sandbox/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  host: localhost\n  port: 5432\n  user: postgres\n  database: postgres\n  schema: postgres\n  search_path: None\n  keepalives_idle: 0\n  sslmode: None\n  Connection test: [OK connection ok]\n\nAll checks passed!\n</code></pre>","title":"dbt debug"},{"location":"002/#dbt-run","text":"<ul> <li>dbt run</li> <li>7.7. VALUES Lists</li> </ul> models/nums.sql<pre><code>SELECT *\nFROM (VALUES (1, 'one'), (2, 'two'), (3, 'three')) AS t (num,letter)\n</code></pre> <pre><code>$ dbt run\nFound 1 model, 0 tests, 0 snapshots, 0 analyses, 167 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\nConcurrency: 1 threads (target='dev')\n1 of 1 START view model postgres.nums .......................................... [RUN]\n1 of 1 OK created view model postgres.nums ..................................... [CREATE VIEW in 0.06s]\nFinished running 1 view model in 0.16s.\nCompleted successfully\nDone. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n</code></pre>","title":"dbt run"},{"location":"003/","text":"","title":"Day 3: sbt-spark plugin"},{"location":"003/#dbt-init","text":"<p>It appears that the entry point to dbt is dbt init (not a directory with some random files that I created initially and have been using so far):</p>  <p><code>dbt init</code> helps get you started using dbt Core!</p>  <p>Let's find out what happens to my existing <code>dbt-sandbox</code> project.</p> <pre><code>$ dbt init\nSetting up your profile.\nThe profile postgres_at_my_company already exists in /Users/jacek/.dbt/profiles.yml. Continue and overwrite it? [y/N]: N\n</code></pre> <p>Let's find out what happens when <code>dbt init</code> from some random directory.</p> <pre><code>$ dbt init\nEnter a name for your project (letters, digits, underscore): hello_dbt\nWhich database would you like to use?\n[1] databricks\n[2] spark\n\n(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)\n\nEnter a number:\n</code></pre> <p>Ouch, I forgot to use <code>pyenv virtualenv dbt</code> to set up a virtual environment with the plugin adapters.</p> <p>Let's start over (remembering the steps from Day 1: Installing dbt).</p> <pre><code>$ pyenv shell dbt\n\n$ pip list dbt | egrep \"^dbt-\" | sort\ndbt-core                 1.1.1\ndbt-databricks           1.1.0\ndbt-extractor            0.4.1\ndbt-postgres             1.1.1\ndbt-spark                1.1.0\n\n$ dbt init\nEnter a name for your project (letters, digits, underscore): hello_dbt\nWhich database would you like to use?\n[1] databricks\n[2] postgres\n[3] spark\n\n(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)\n\nEnter a number: 3\nhost (yourorg.sparkhost.com): localhost\n[1] odbc\n[2] http\n[3] thrift\nDesired authentication method option (enter a number): 2\ntoken (abc123): abc123\nconnect_timeout [10]:\nconnect_retries [0]:\nport [443]:\nschema (default schema that dbt will build objects in): my_schema\nthreads (1 or more) [1]:\nProfile hello_dbt written to /Users/jacek/.dbt/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.\nYour new dbt project \"hello_dbt\" was created!\n\nFor more information on how to configure the profiles.yml file,\nplease consult the dbt documentation here:\n\n  https://docs.getdbt.com/docs/configure-your-profile\n\nOne more thing:\n\nNeed help? Don't hesitate to reach out to us via GitHub issues or on Slack:\n\n  https://community.getdbt.com/\n\nHappy modeling!\n\n$ tree hello_dbt\nhello_dbt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 analyses\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 macros\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 example\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 my_first_dbt_model.sql\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 my_second_dbt_model.sql\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 schema.yml\n\u251c\u2500\u2500 seeds\n\u251c\u2500\u2500 snapshots\n\u2514\u2500\u2500 tests\n\n7 directories, 5 files\n</code></pre> <p>The most important take away is the following message:</p>  <p>Profile hello_dbt written to /Users/jacek/.dbt/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.</p>  <p>We learnt <code>dbt debug</code> the other day already!</p> <pre><code>$ cd hello_dbt\n\n$ dbt debug\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /private/tmp/hello_dbt/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [ERROR invalid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\n1 check failed:\nProfile loading failed for the following reason:\nRuntime Error\n  Credentials in profile \"hello_dbt\", target \"dev\" invalid: Runtime Error\n    http connection method requires additional dependencies.\n    Install the additional required dependencies with `pip install dbt-spark[PyHive]`\n</code></pre> <p>I had to quote <code>dbt-spark[PyHive]</code> to get it working on my macOS on oh-my-zsh. Wonder what shells would unquoted square brackets work?!</p> <pre><code>$ pip install \"dbt-spark[PyHive]\"\n...\nSuccessfully installed PyHive-0.6.5 pure-sasl-0.6.2 sasl-0.3.1 thrift-0.15.0 thrift_sasl-0.4.3\n\n$ dbt debug\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /private/tmp/hello_dbt/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  host: localhost\n  port: 443\n  cluster: None\n  endpoint: None\n  schema: my_schema\n  organization: 0\n  Connection test: [ERROR]\n\n1 check failed:\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n\n  &gt;Runtime Error\n  Database Error\n    failed to connect\n\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\n</code></pre>","title":"dbt init"},{"location":"003/#dbt-spark","text":"<p>dbt-spark</p> <pre><code>$ dbt --version\nCore:\n  - installed: 1.1.1\n  - latest:    1.1.1 - Up to date!\n\nPlugins:\n  - databricks: 1.1.0 - Up to date!\n  - postgres:   1.1.1 - Up to date!\n  - spark:      1.1.0 - Up to date!\n</code></pre> <p>Following the official Getting started.</p> <pre><code>$ git clone https://github.com/dbt-labs/dbt-spark.git\n\n$ cd dbt-spark\n\n$ docker-compose up\n...\ndbt-spark-dbt-spark3-thrift-1   | 22/07/04 13:35:18 INFO HiveThriftServer2: HiveThriftServer2 started\n</code></pre> <p>http://localhost:4040/sqlserver/ should work fine.</p> <p></p> <pre><code>$ dbt init\nSetting up your profile.\nThe profile hello_dbt already exists in /Users/jacek/.dbt/profiles.yml. Continue and overwrite it? [y/N]: y\nWhich database would you like to use?\n[1] databricks\n[2] postgres\n[3] spark\n\n(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)\n\nEnter a number: 3\nhost (yourorg.sparkhost.com): localhost\n[1] odbc\n[2] http\n[3] thrift\nDesired authentication method option (enter a number): 3\nport [443]: 10000\nschema (default schema that dbt will build objects in): analytics\nthreads (1 or more) [1]:\nProfile hello_dbt written to /Users/jacek/.dbt/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.\n\n$ dbt debug\n13:42:50  Running with dbt=1.1.1\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /private/tmp/hello_dbt/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  host: localhost\n  port: 10000\n  cluster: None\n  endpoint: None\n  schema: analytics\n  organization: 0\n  Connection test: [OK connection ok]\n\nAll checks passed!\n</code></pre> <p>Yay! dbt on Spark finally! That concludes Day 3.</p>","title":"dbt-spark"},{"location":"004/","text":"<p>It turns out that I used a fake dbt project (<code>/tmp/hello_dbt</code>) with <code>pyenv shell dbt</code> the other day. It has to be fixed some day.</p> <p>Before it happens, I couldn't wait to have a look at Spark Thrift Server that I completely forgot about.</p> <p>This is what dbt-spark project uses in docker-compose.yml.</p>","title":"Day 4: Spark Thrift Server (and dbt debug)"},{"location":"004/#running-spark-thrift-server","text":"<p>Unless I'm missing something, there's no reason why they keep using Apache Spark 3.1.1 (as I learnt on Day 3). The latest and greatest 3.3.0 works just fine.</p> <pre><code>$ ./bin/spark-submit --version\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n      /_/\n\nUsing Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.15\nBranch heads/v3.3.0\nCompiled by user jacek on 2022-06-17T18:35:32Z\nRevision f74867bddfbcdd4d08076db36851e88b15e66556\nUrl https://github.com/apache/spark.git\nType --help for more information.\n</code></pre> <p>Let's start Spark Thrift Server.</p> <pre><code>./sbin/start-thriftserver.sh\n</code></pre> <p>Observe the logs in <code>logs</code> directory.</p> <pre><code>$ tail -f logs/spark-jacek-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2*.out\n...\nINFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads\nINFO AbstractService: Service:HiveServer2 is started.\nINFO HiveThriftServer2: HiveThriftServer2 started\n</code></pre> <p>Use http://localhost:4040/sqlserver/ to monitor queries.</p> <pre><code>$ pwd\n/tmp/hello_dbt\n\n$ dbt debug\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /private/tmp/hello_dbt/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  host: localhost\n  port: 10000\n  cluster: None\n  endpoint: None\n  schema: analytics\n  organization: 0\n  Connection test: [OK connection ok]\n\nAll checks passed!\n</code></pre> <p>With that, you can use whatever Spark version you want and skip docker-compose.yml.</p> <p>The following shows the queries that <code>dbt debug</code> uses to check connection.</p> <p></p> <p>There are the following two queries:</p> <pre><code>USE `default`\n</code></pre> <pre><code>select 1 as id\n</code></pre>","title":"Running Spark Thrift Server"},{"location":"005/","text":"<p>The very first sentence in dbt Models reminds me the days of Hibernate and ORMs in general where <code>SELECT</code>s were mapped to Java bean classes (for programmers to work with relational entities as if they were Java types) to build...a model. In dbt, it's no longer needed since it's all about pure SQL (!)</p>  <p>A model is a <code>select</code> statement.</p>","title":"Day 5: dbt Models"},{"location":"005/#set-up-dbt-project","text":"<pre><code>$ pyenv shell dbt\n\n$ dbt init\nEnter a name for your project (letters, digits, underscore): hello_dbt_spark\nWhich database would you like to use?\n[1] databricks\n[2] postgres\n[3] spark\n\n(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)\n\nEnter a number: 3\n...\n\n$ tree hello_dbt_spark\nhello_dbt_spark\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 analyses\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 macros\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 example\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 my_first_dbt_model.sql\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 my_second_dbt_model.sql\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 schema.yml\n\u251c\u2500\u2500 seeds\n\u251c\u2500\u2500 snapshots\n\u2514\u2500\u2500 tests\n\n7 directories, 5 files\n</code></pre>","title":"Set Up dbt Project"},{"location":"005/#start-spark-thrift-server","text":"<pre><code>$ cd $SPARK_HOME\n\n$ ./sbin/start-thriftserver.sh\nstarting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /Users/jacek/dev/oss/spark/logs/spark-jacek-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-japila-new.local.out\n\n$ tail -f logs/spark-jacek-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-japila-new.local.out\n...\nINFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads\nINFO AbstractService: Service:HiveServer2 is started.\nINFO HiveThriftServer2: HiveThriftServer2 started\n</code></pre>","title":"Start Spark Thrift Server"},{"location":"005/#pyenv-local-dbt","text":"<p>Let's switch to the directory and make sure that <code>dbt</code> virtualenv is there forever.</p> <pre><code>$ cd hello_dbt_spark\n$ pyenv local dbt\n</code></pre> <p>Just as a friendly reminder (to myself), the above <code>pyenv</code> command creates <code>.python-version</code> file with the name of the virtualenv.</p> <pre><code>$ cat .python-version\ndbt\n</code></pre>","title":"pyenv local dbt"},{"location":"005/#dbt-debug","text":"<pre><code>$ dbt debug\ndbt version: 1.1.1\npython version: 3.9.13\npython path: /Users/jacek/.pyenv/versions/dbt/bin/python3.9\nos info: macOS-11.6.6-x86_64-i386-64bit\nUsing profiles.yml file at /Users/jacek/.dbt/profiles.yml\nUsing dbt_project.yml file at /Users/jacek/dev/sandbox/hello_dbt_spark/dbt_project.yml\n\nConfiguration:\n  profiles.yml file [OK found and valid]\n  dbt_project.yml file [OK found and valid]\n\nRequired dependencies:\n - git [OK found]\n\nConnection:\n  host: localhost\n  port: 10000\n  cluster: None\n  endpoint: None\n  schema: analytics\n  organization: 0\n  Connection test: [OK connection ok]\n\nAll checks passed!\n</code></pre>","title":"dbt debug"},{"location":"005/#dbt-run","text":"<p>From dbt Models:</p>  <p>When you execute the <code>dbt run</code> command, dbt will build this model in your data warehouse by wrapping it in a <code>create view as</code> or <code>create table as</code> statement.</p>  <pre><code>$ dbt run\nRunning with dbt=1.1.1\nPartial parse save file not found. Starting full parse.\nFound 2 models, 4 tests, 0 snapshots, 0 analyses, 199 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n\nConcurrency: 1 threads (target='dev')\n\n1 of 2 START table model analytics.my_first_dbt_model .......................... [RUN]\n1 of 2 OK created table model analytics.my_first_dbt_model ..................... [OK in 1.45s]\n2 of 2 START view model analytics.my_second_dbt_model .......................... [RUN]\n2 of 2 OK created view model analytics.my_second_dbt_model ..................... [OK in 0.19s]\n\nFinished running 1 table model, 1 view model in 2.54s.\n\nCompleted successfully\n\nDone. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n</code></pre>","title":"dbt run"},{"location":"005/#beeline","text":"<pre><code>$ cd $SPARK_HOME\n\n$ ./bin/beeline -u jdbc:hive2://localhost:10000/analytics\nConnecting to jdbc:hive2://localhost:10000/analytics\nConnected to: Spark SQL (version 3.3.0)\nDriver: Hive JDBC (version 2.3.9)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\nBeeline version 2.3.9 by Apache Hive\n0: jdbc:hive2://localhost:10000/analytics&gt; show tables;\n+------------+----------------------+--------------+\n| namespace  |      tableName       | isTemporary  |\n+------------+----------------------+--------------+\n| analytics  | my_first_dbt_model   | false        |\n| analytics  | my_second_dbt_model  | false        |\n+------------+----------------------+--------------+\n2 rows selected (0.119 seconds)\n0: jdbc:hive2://localhost:10000/analytics&gt; show views;\n+------------+----------------------+--------------+\n| namespace  |       viewName       | isTemporary  |\n+------------+----------------------+--------------+\n| analytics  | my_second_dbt_model  | false        |\n+------------+----------------------+--------------+\n1 row selected (0.048 seconds)\n</code></pre>","title":"beeline"},{"location":"005/#how-can-i-see-the-sql-that-dbt-is-running","text":"<p>From FAQs:</p> <ul> <li>The <code>target/compiled/</code> directory for compiled select statements</li> <li>The <code>target/run/</code> directory for compiled create statements</li> <li>The <code>logs/dbt.log</code> file for verbose logging.</li> </ul>","title":"How can I see the SQL that dbt is running?"},{"location":"005/#stop-spark-thrift-server","text":"<pre><code>$ cd $SPARK_HOME\n\n$ ./sbin/stop-thriftserver.sh\nstopping org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\n</code></pre>","title":"Stop Spark Thrift Server"},{"location":"006/","text":"<p>From Tests:</p>  <p>Tests are assertions you make about your models and other resources in your dbt project (e.g. sources, seeds and snapshots). When you run <code>dbt test</code>, dbt will tell you if each test in your project passes or fails.</p> <p>tests are SQL queries.</p> <p>select statements that seek to grab \"failing\" records</p>  <p>Examples of asserts:</p> <ul> <li>a column is unique in a model</li> <li>a column is never null</li> </ul>","title":"Day 6: dbt test"},{"location":"006/#dbt-test","text":"<pre><code>$ dbt test\nRunning with dbt=1.1.1\nFound 2 models, 4 tests, 0 snapshots, 0 analyses, 199 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n\nConcurrency: 1 threads (target='dev')\n\n1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]\n1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [FAIL 1 in 0.72s]\n2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]\n2 of 4 PASS not_null_my_second_dbt_model_id .................................... [PASS in 0.31s]\n3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]\n3 of 4 PASS unique_my_first_dbt_model_id ....................................... [PASS in 0.54s]\n4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]\n4 of 4 PASS unique_my_second_dbt_model_id ...................................... [PASS in 0.37s]\n\nFinished running 4 tests in 2.28s.\n\nCompleted with 1 error and 0 warnings:\n\nFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)\n  Got 1 result, configured to fail if != 0\n\n  compiled SQL at target/compiled/hello_dbt_spark/models/example/schema.yml/not_null_my_first_dbt_model_id.sql\n\nDone. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4\n</code></pre>","title":"dbt test"},{"location":"006/#store-failures","text":"<p>Spark Thrift Server uses 5 threads to handle requests so I changed <code>~/.dbt/profiles.yml</code> to match it.</p> ~/.dbt/profiles.yml<pre><code>local_spark_thrift_server:\n  outputs:\n    dev:\n      host: localhost\n      method: thrift\n      port: 10000\n      schema: analytics\n      threads: 5\n      type: spark\n  target: dev\n</code></pre> <pre><code>$ dbt test --store-failures\nRunning with dbt=1.1.1\nFound 2 models, 4 tests, 0 snapshots, 0 analyses, 199 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n\nConcurrency: 5 threads (target='dev')\n\n1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]\n2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]\n3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]\n4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]\n4 of 4 PASS unique_my_second_dbt_model_id ...................................... [PASS in 1.97s]\n1 of 4 FAIL 1 not_null_my_first_dbt_model_id ................................... [FAIL 1 in 1.98s]\n2 of 4 PASS not_null_my_second_dbt_model_id .................................... [PASS in 2.13s]\n3 of 4 PASS unique_my_first_dbt_model_id ....................................... [PASS in 2.15s]\n\nFinished running 4 tests in 2.76s.\n\nCompleted with 1 error and 0 warnings:\n\nFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)\nGot 1 result, configured to fail if != 0\n\ncompiled SQL at target/compiled/hello_dbt_spark/models/example/schema.yml/not_null_my_first_dbt_model_id.sql\n\nSee test failures:\n----------------------------------------------------------------------\nselect * from analytics_dbt_test__audit.not_null_my_first_dbt_model_id\n----------------------------------------------------------------------\n\nDone. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4\n</code></pre> <pre><code>0: jdbc:hive2://localhost:10000/analytics&gt; select * from analytics_dbt_test__audit.not_null_my_first_dbt_model_id;\n+-------+\n|  id   |\n+-------+\n| NULL  |\n+-------+\n1 row selected (0.091 seconds)\n</code></pre>","title":"store-failures"},{"location":"follow-ups/","text":"<ol> <li>Review docker-compose.yml from <code>dbt-spark</code> adapter<ul> <li>How are all things composed together?</li> <li>Spark 3.1.1 only?! Why not using the latest 3.3.0?</li> </ul> </li> <li>Use spark-testing profile from<code>dbt-spark</code> adapter</li> <li>Profile Setup for the dbt-databricks plugin</li> <li>Review CONTRIBUTING</li> <li>Read up on dbt init (esp. profile_template.yml)</li> </ol>","title":"Follow-ups"},{"location":"learning-resources/","text":"","title":"Learning Resources"},{"location":"learning-resources/#videos","text":"<ol> <li>How to build a mature dbt project from scratch</li> </ol>","title":"Videos"},{"location":"overview/","text":"","title":"About dbt"},{"location":"overview/#introduction","text":"<p>We built dbt to help advance the practice of analytics engineering: a workflow that allows anyone who knows SQL to bring engineering practices to analytics, and to build datasets that are accurate, tested, and documented.</p>","title":"Introduction"},{"location":"overview/#courses","text":"<p>Courses</p> <ul> <li>dbt Fundamentals</li> </ul>","title":"Courses"},{"location":"overview/#review","text":"<ol> <li>https://github.com/dbt-labs/dbt-core</li> <li>https://github.com/dbt-labs/dbt-spark</li> <li>https://github.com/databricks/dbt-databricks</li> </ol>","title":"Review"}]}